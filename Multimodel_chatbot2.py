import base64
import uuid
import io
import gradio as gr
from PIL import Image
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.messages import HumanMessage
from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate
from langchain_core.runnables import RunnableWithMessageHistory
from new_langchaing_practice.models import multimodal_llm

prompt = ChatPromptTemplate([
    ("system", "You are a multimodal AI assistant. You can take test, voice and image input."),
    MessagesPlaceholder(variable_name = "messages"),

])

chain = prompt | multimodal_llm

def get_session_history(session_id : str):
    return SQLChatMessageHistory(
        session_id = session_id,
        connection = "sqlite:///chat_history.db"
    )

chain_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
)

config = {"configurable" : {"session_id" : str(uuid.uuid4())}}

def transcribe_audio(audio_path):
    """ä½¿ç”¨Base64å¤„ç†è¯­éŸ³è½¬ä¸º"""
    # ç›®å‰å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼š æ”¯æŒä¸¤ä¸ªä¼ å‚æ–¹å¼ï¼Œ1ã€base64ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼ˆæœ¬åœ°ï¼‰ã€‚2ã€ç½‘ç»œè®¿é—®çš„urlåœ°å€ï¼ˆå¤–ç½‘çš„æœåŠ¡å™¨ä¸Šï¼‰ http://sxxxx.com/11.mp3
    try:
        with open(audio_path, 'rb') as audio_file:
            audio_data = base64.b64encode(audio_file.read()).decode('utf-8')
        # Qwen model format:
        # audio_message = {  # æŠŠéŸ³é¢‘æ–‡ä»¶ï¼Œå°è£…æˆä¸€æ¡æ¶ˆæ¯
        #     "type": "audio_url",
        #     "audio_url": {
        #         "url": f"data:audio/wav;base64,{audio_data}",
        #         "duration": 30  # å•ä½ï¼šç§’ï¼ˆå¸®åŠ©æ¨¡å‹ä¼˜åŒ–å¤„ç†ï¼‰
        #     }
        # }

        audio_message = {  # æŠŠéŸ³é¢‘æ–‡ä»¶ï¼Œå°è£…æˆä¸€æ¡æ¶ˆæ¯
            "type": "audio",
            "audio": f"data:audio/wav;base64,{audio_data}",
        }
        return audio_message
    except Exception as e:
        print(e)
        return {}


def transcribe_image(image_path):
    """
    å°†ä»»æ„æ ¼å¼çš„å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç çš„data URL
    :param image_path: å›¾ç‰‡è·¯å¾„
    :return: åŒ…å«base64ç¼–ç çš„å­—å…¸
    """
    with Image.open(image_path) as img:
        # è·å–åŸå§‹å›¾ç‰‡æ ¼å¼ï¼ˆå¦‚JPEG/PNGï¼‰
        img_format = img.format if img.format else 'JPEG'

        buffered = io.BytesIO()
        # ä¿ç•™åŸå§‹æ ¼å¼ï¼ˆé¿å…JPEGå¼ºåˆ¶è½¬æ¢å¯¼è‡´é€æ˜é€šé“ä¸¢å¤±ï¼‰
        img.save(buffered, format=img_format)

        image_data = base64.b64encode(buffered.getvalue()).decode('utf-8')
        return {
            "type": "image",
            "image": f"data:image/{img_format.lower()};base64,{image_data}"
        }

def add_message(history, messages):
    """
    Add user's message to chat history
    """
    for m in messages["files"]:
        print(m)
        history.append({"role": "user", "content": {"path": m}})
    if messages["text"] is not None:
        history.append({"role": "user", "content": messages["text"]})
    return history, gr.MultimodalTextbox(value=None, interactive=False)

def get_last_user_after_assistant(history):
    """Find last assistant's location and return all user messages after"""
    if not history:
        return None
    if history[-1]["role"] == "assistant":
        return None

    last_assistant_idx = -1
    for i in range(len(history) - 1, -1, -1):
        if history[i]["role"] == "assistant":
            last_assistant_idx = i
            break
    # if assistant is not found
    if last_assistant_idx == -1:
        return history
    else:
        # first user after assistant
        return history[last_assistant_idx + 1:]

# def submit_messages(history):
#     """
#     Submit user input, generate chatbot reply
#     """
#     user_messages = get_last_user_after_assistant(history)
#     print(user_messages)
#     content = []
#     if user_messages:
#         for x in user_messages:
#             if isinstance(x["content"], str): # this means the content is text
#                 content.append(x["content"])
#             elif isinstance(x["content"], tuple): #multimeida content
#                 file_path = x ["content"][0] # get the multimedia file's path
#                 if file_path.endswith(".wav"): # it is a audio file
#                     file_message = transcribe_audio(file_path)
#                 elif file_path.endswith(".jpg") or file_path.endswith(".png") or file_path.endswith(".jpeg"):
#                     file_message = transcribe_image(file_path)
#                 content.append(file_message)
#             else:
#                 pass
#     input_message = HumanMessage(content = content)
#
#     resp = chain_history.invoke({"messages" : input_message}, config)
#     history.append({"role" : "assistant", "content" : resp.content})
#
#     return history
def submit_messages(history):
    user_messages = get_last_user_after_assistant(history)
    if not user_messages:
        return history

    content = []

    # Extract text and files
    text_parts = []
    media_parts = []

    for x in user_messages:
        if isinstance(x["content"], str) and x["content"].strip():
            text_parts.append(x["content"])
        elif isinstance(x["content"], tuple):
            file_path = x["content"][0]
            if file_path.endswith(".wav"):
                msg = transcribe_audio(file_path)
                if msg:
                    media_parts.append(msg)
            elif file_path.endswith((".jpg", ".png", ".jpeg")):
                msg = transcribe_image(file_path)
                if msg:
                    media_parts.append(msg)

    # âœ… Always include descriptive text
    if media_parts:
        if not text_parts:
            # ğŸ›  Add fallback text if user didn't type anything
            text_parts.append("Please analyze this audio.")
        # âœ… Append text first, then media
        content.extend(text_parts)
        content.extend(media_parts)
    else:
        # Only text
        content.extend(text_parts)

    input_message = HumanMessage(content=content)

    try:
        resp = chain_history.invoke(
            {"messages": [input_message]},
            config=config
        )
        history.append({"role": "assistant", "content": resp.content})
    except Exception as e:
        print("LLM invocation error:", e)
        history.append({"role": "assistant", "content": "Sorry, I couldn't process your request."})

    return history


# user_msg: HumanMessage = HumanMessage(content = [{"type":"text", "text" : "What's the highest mountain in the world?"}])
#
# resp1 = chain_history.invoke({"messages" : [user_msg]}, config)
# print(resp1.content)

#TODO: use Gradio to develop a GUI for the chatbot
with gr.Blocks(title = "Multimodal Chatbot", theme = gr.themes.Soft()) as block:
    # chat history
    chatbot = gr.Chatbot(type = "messages", height = 500, label = "Chatbot")

    # multimodal input
    chat_input = gr.MultimodalTextbox(
        interactive = True,
        file_types=['image', '.wav', '.mp4'],
        file_count = "multiple",
        placeholder = "Please input information or upload files...",
        show_label = False,
        sources=["microphone", "upload"],
    )

    chat_input.submit(
        add_message,
        [chatbot, chat_input],
        [chatbot, chat_input]
    ).then(
        submit_messages,
        [chatbot],
        [chatbot]
    ).then(
        lambda: gr.MultimodalTextbox(interactive = True), # reset the chat window
        None,
        [chat_input]
    )

if __name__ == "__main__":
    block.launch()