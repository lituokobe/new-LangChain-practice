from typing import Optional

from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser, SimpleJsonOutputParser
from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate, MessagesPlaceholder, ChatPromptTemplate, \
    FewShotChatMessagePromptTemplate
from pydantic import BaseModel, Field

from new_langchaing_practice.models import llm


# TODO: Let LLM answer the question with different style by adjusting the system message
# message = [
#     # ("system", "You are a smart assistant, and you answer question in a witty way."),
#     # ("system", "You are a serious assistant, and you answer question in an academic way."),
#     ("system", "You are an innocent cute baby, and you don't know too many things, but you answer question in a childish way."),
#     ("human", "Why is the sky blue?")
# ]
#
# resp = llm.invoke(message)
#
# print(resp.content)

# TODO: Use prompt template - variable placeholder
# prompt_template = PromptTemplate.from_template("Help me generate a story of {subject}.")
# chain = prompt_template | llm
#
# resp = chain.invoke({"subject" : "cute panda"})
#
# print(resp.content)

# TODO: FewShotPromptTemplate
# examples = [
#     {
#         "question": "ç©†ç½•é»˜å¾·Â·é˜¿é‡Œå’Œè‰¾ä¼¦Â·å›¾çµè°æ´»å¾—æ›´ä¹…ï¼Ÿ",
#         "answer": """
# æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯ã€‚
# åç»­é—®é¢˜ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶å¤šå¤§ï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶74å²ã€‚
# åç»­é—®é¢˜ï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶å¤šå¤§ï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶41å²ã€‚
# æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œ
# """,
#     },
#     {
#         "question": "ä¹”æ²»Â·åç››é¡¿çš„å¤–ç¥–çˆ¶æ˜¯è°ï¼Ÿ",
#         "answer": """
# æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯ã€‚
# åç»­é—®é¢˜ï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯è°ï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯ç›ä¸½Â·é²å°”Â·åç››é¡¿ã€‚
# åç»­é—®é¢˜ï¼šç›ä¸½Â·é²å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°ï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šç›ä¸½Â·é²å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯çº¦ç‘Ÿå¤«Â·é²å°”ã€‚
# æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šçº¦ç‘Ÿå¤«Â·é²å°”
# """,
#     },
#     {
#         "question": "ã€Šå¤§ç™½é²¨ã€‹å’Œã€Š007ï¼šå¤§æˆ˜çš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯å¦æ¥è‡ªåŒä¸€ä¸ªå›½å®¶ï¼Ÿ",
#         "answer": """
# æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯ã€‚
# åç»­é—®é¢˜ï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯å²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼ã€‚
# åç»­é—®é¢˜ï¼šå²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼æ¥è‡ªå“ªé‡Œï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šç¾å›½ã€‚
# åç»­é—®é¢˜ï¼šã€Š007ï¼šå¤§æˆ˜çš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šã€Š007ï¼šå¤§æˆ˜çš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯é©¬ä¸Â·åè´å°”ã€‚
# åç»­é—®é¢˜ï¼šé©¬ä¸Â·åè´å°”æ¥è‡ªå“ªé‡Œï¼Ÿ
# ä¸­é—´ç­”æ¡ˆï¼šæ–°è¥¿å…°ã€‚
# æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šå¦
# """,
#     },
# ]
# base_template = PromptTemplate.from_template("Question:{question}\n{answer}")
# final_template = FewShotPromptTemplate(
#     examples = examples,
#     example_prompt = base_template,
#     suffix = "Question: {input}",
#     input_variables = ["input"]
# )
#
# chain = final_template | llm
# resp = chain.invoke({"input" : "ä¸­å›½å†å²ä¸Šï¼Œéš‹æœå’Œç§¦æœå“ªä¸ªæŒç»­çš„æ—¶é—´æ¯”è¾ƒé•¿ï¼Ÿ"})
#
# print(resp)

# TODO: Use prompt template - message placeholder
# prompt_template = ChatPromptTemplate.from_messages([
#     ("system", "You are a basketball fan, and you favorite player is Kobe Bryant, you answer all the questions from your passion of basketball and Kobe Bryant."),
#     MessagesPlaceholder("input")
# ]
# )
# chain = prompt_template | llm
#
# resp = chain.invoke({"input" : [HumanMessage(content = "How to study math?")]})
#
# print(resp.content)

# TODO: ICL in chat template
# examples = [
#     {"input" : "2 ğŸ¦œ 2", "output" : "5"},
#     {"input" : "3 ğŸ¦œ 2", "output" : "7"},
#     {"input" : "4 ğŸ¦œ 6", "output" : "25"}
# ]
#
# base_prompt = ChatPromptTemplate.from_messages(
#     [
#         ("human", "{input}"),
#         ("ai", "{output}")
#     ]
# )
#
# few_shot_prompt = FewShotChatMessagePromptTemplate(
#     examples = examples,
#     example_prompt = base_prompt
# )
#
# prompt_template = ChatPromptTemplate.from_messages(
#     [
#         ("system", "You are a smart AI assistant. " "The following examples show how to interpret ğŸ¦œ."),
#         # Must ask the model to learn from the examples
#         few_shot_prompt,
#         MessagesPlaceholder("msg")
#     ]
# )
#
# chain = prompt_template | llm | StrOutputParser()
#
# resp = chain.invoke({"msg" : [HumanMessage(content = "What is 5 ğŸ¦œ 8?")]})
#
# print(resp)

# TODO output parser

# prompt_template = ChatPromptTemplate.from_messages(
#     [
#         ("system", "You are a smart AI assistant. "),
#         MessagesPlaceholder("msg")
#     ]
# )
#
# chain = prompt_template | llm | StrOutputParser()
# # StrOutputParser will only return the string of the model output
#
# resp = chain.invoke({"msg" : [HumanMessage(content = "Can you tell me a joke about Japanese? ")]})
#
# print(resp)

# TODO: structured output
# class Joke(BaseModel):
#     """
#     BaseModel is a data model class
#     """
#     setup : str = Field(description = "beginning of the joke")
#     punchline: str = Field(description="the fun part of the joke")
#     rating: Optional[str] = Field(description="the level of fun of the joke, from 1 to 10")
#
# prompt_template = PromptTemplate.from_template("Generate a joke about {topic}.")
# runnable = llm.with_structured_output(Joke)
#
# chain = prompt_template | runnable
#
# resp = chain.invoke({"topic" : "Indians"})
#
# print(resp)
# print(resp.__dict__)

# TODO: SimpleJsonOutputParser
# prompt_template = PromptTemplate.from_template(
#     "Try your best to answer the user's question"
#     "You must output a JSON object that includes keys of 'answer' and 'followup_question', 'answer' is the reply to user's question, 'followup_question' is the possible question that user may ask next."
#     "{question}"
# )
#
# chain = prompt_template | llm | SimpleJsonOutputParser()
#
# resp = chain.invoke({"question" : "What makes a man attractive?"})
#
# print(resp)

# TODO .bind_tools to structure the output
# This requires high-level of natural language understanding and small models like gpt-4.1-nano doesn't do it well.
class ResponseFormatter(BaseModel):
    """Use this tool to structure your response to the user"""

    answer: str = Field(description="response to the user")
    followup_question: str = Field(description="the question that the users may ask next.")

runnable = llm.bind_tools([ResponseFormatter])

resp = runnable.invoke("Which country uses most electricity in the world?")
# print(resp.tool_calls[-1]['args'])
print(resp)
resp.pretty_print()